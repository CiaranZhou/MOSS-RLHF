# MOSS-RLHF

## *MOSS-RLHF & "Secrets of RLHF in Large Language Models Part II: PPO"*

<p align="center" width="100%">
<img src="./assets/img/moss.png" alt="MOSS" style="width: 50%; min-width: 300px; display: block; margin: auto;">

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-brightgreen.svg)](./LICENSE)
[![Data License](https://img.shields.io/badge/Data%20License-CC%20BY--NC%204.0-blue.svg)](./DATA_LICENSE)
[![Model License](https://img.shields.io/badge/Model%20License-GNU%20AGPL%203.0-red.svg)](./MODEL_LICENSE)

This is the open-source code repository for the technical reports: "Secrets of RLHF in Large Language Models Part II: PPO"

<img style="width: 90%; min-width: 500px; display: block; margin: auto; margin-bottom: 20px" alt="MOSS-RLHF" src="./assets/img/img1.jpg">


## Open-source List
- Two 7B reward model based on openChineseLlama and Llama-7B, respectively.
- Open source code for RL training in large language models.
- ...

## Getting Started

TODO, To be finalised before 15. July 2023
